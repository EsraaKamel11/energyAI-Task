2025-07-26 15:57:14,597 - INFO - ml_pipeline.src.training_qlora.experiment_tracker - Started experiment: qlora_microsoft/DialoGPT-medium_20250726_155706
2025-07-26 15:57:14,597 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - ✅ Experiment tracking started with W&B
2025-07-26 15:57:14,603 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - 📥 Step 1: Loading model with quantization...
2025-07-26 15:57:14,603 - INFO - ml_pipeline.src.training_qlora.model_loader - Loading model microsoft/DialoGPT-medium with 4bit quantization
2025-07-26 15:57:15,450 - INFO - accelerate.utils.modeling - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-07-26 15:57:21,176 - INFO - ml_pipeline.src.training_qlora.model_loader - Model prepared for k-bit training
2025-07-26 15:57:21,176 - INFO - ml_pipeline.src.training_qlora.model_loader - ✅ Model microsoft/DialoGPT-medium loaded successfully
2025-07-26 15:57:21,187 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - Model info: {'total_parameters': 203828224, 'trainable_parameters': 0, 'model_size_mb': 777.54296875, 'device': 'cuda:0'}
2025-07-26 15:57:21,189 - INFO - ml_pipeline.src.training_qlora.experiment_tracker - Logged model information
2025-07-26 15:57:21,190 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - 🎯 Step 2: Setting up QLoRA configuration...
2025-07-26 15:57:21,191 - INFO - ml_pipeline.src.training_qlora.lora_config - Created LoRA config: r=16, alpha=32, targets=['c_attn', 'c_proj']
2025-07-26 15:57:21,191 - INFO - ml_pipeline.src.training_qlora.lora_config - Applying LoRA configuration to model...
trainable params: 4,325,376 || all params: 359,148,544 || trainable%: 1.2043
2025-07-26 15:57:21,425 - INFO - ml_pipeline.src.training_qlora.lora_config - ✅ LoRA configuration applied successfully
2025-07-26 15:57:21,438 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - LoRA info: {'trainable_parameters': 4325376, 'all_parameters': 208153600, 'trainable_percentage': 2.0779731890296396}
2025-07-26 15:57:21,440 - INFO - ml_pipeline.src.training_qlora.experiment_tracker - Logged training configuration
2025-07-26 15:57:21,441 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - 📊 Step 3: Preparing training data...
2025-07-26 15:57:21,441 - INFO - ml_pipeline.src.training_qlora.data_preparation - Loading QA data from ml_pipeline/pipeline_output/qa_pairs.jsonl
2025-07-26 15:57:21,448 - INFO - ml_pipeline.src.training_qlora.data_preparation - Loaded 3 QA pairs
2025-07-26 15:57:21,448 - INFO - ml_pipeline.src.training_qlora.data_preparation - Created 3 conversations
2025-07-26 15:57:21,515 - INFO - ml_pipeline.src.training_qlora.data_preparation - Train samples: 2
2025-07-26 15:57:21,517 - INFO - ml_pipeline.src.training_qlora.data_preparation - Validation samples: 1
Map: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 71.38 examples/s]
2025-07-26 15:57:21,602 - INFO - ml_pipeline.src.training_qlora.data_preparation - Tokenized dataset with max_length=1024
Map: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 294.54 examples/s]
2025-07-26 15:57:21,647 - INFO - ml_pipeline.src.training_qlora.data_preparation - Tokenized dataset with max_length=1024
2025-07-26 15:57:21,654 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - Train stats: {'num_samples': 2, 'avg_token_length': 1024.0, 'min_token_length': 1024, 'max_token_length': 1024, 'total_tokens': 2048}
2025-07-26 15:57:21,656 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - Validation stats: {'num_samples': 1, 'avg_token_length': 1024.0, 'min_token_length': 1024, 'max_token_length': 1024, 'total_tokens': 1024}
2025-07-26 15:57:21,656 - INFO - ml_pipeline.src.training_qlora.experiment_tracker - Logged dataset information
2025-07-26 15:57:21,657 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - ⚙️ Step 4: Configuring training...
2025-07-26 15:57:21,657 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - 🚀 Step 5: Starting training...
2025-07-26 15:57:21,658 - INFO - ml_pipeline.src.training_qlora.training_loop - Setting up QLoRA training...
D:\energyAI-Task\ml_pipeline\src\training_qlora\training_loop.py:115: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
2025-07-26 15:57:21,867 - INFO - ml_pipeline.src.training_qlora.training_loop - ✅ Training setup complete
2025-07-26 15:57:21,870 - INFO - ml_pipeline.src.training_qlora.training_loop - 🚀 Starting QLoRA training...
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                            | 0/3 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:48<00:00, 16.27s/it]
{'train_runtime': 48.85, 'train_samples_per_second': 0.123, 'train_steps_per_second': 0.061, 'train_loss': 6.826514561971028, 'epoch': 3.0}
***** train metrics *****
  epoch                    =        3.0
  total_flos               =    10527GF
  train_loss               =     6.8265
  train_runtime            = 0:00:48.85
  train_samples_per_second =      0.123
  train_steps_per_second   =      0.061
2025-07-26 15:58:11,791 - INFO - ml_pipeline.src.training_qlora.training_loop - ✅ Training completed! Model saved to ml_pipeline\pipeline_output\qlora_training\checkpoints\final_model
2025-07-26 15:58:11,794 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - 💾 Step 6: Saving final model...
2025-07-26 15:58:12,409 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - 📋 Step 7: Generating training summary...
2025-07-26 15:58:12,425 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - ✅ QLoRA training pipeline completed successfully!
2025-07-26 15:58:12,426 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - Final model saved to: ml_pipeline\pipeline_output\qlora_training\final_model
2025-07-26 15:58:12,428 - INFO - ml_pipeline.src.training_qlora.main_orchestrator - Training summary saved to: ml_pipeline\pipeline_output\qlora_training\training_summary.json
2025-07-26 15:58:12,429 - INFO - ml_pipeline.src.training_qlora.experiment_tracker - Logged final experiment summary
2025-07-26 15:58:13,061 - INFO - ml_pipeline.src.training_qlora.experiment_tracker - Logged artifact: training_summary
